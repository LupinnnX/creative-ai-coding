# =============================================================================
# Remote Agentic Coding System - Environment Configuration
# Droid (Factory) + GLM Coding Plan + Telegram
# =============================================================================

# Database
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/creative_ai_coding

# =============================================================================
# AI Assistant: Droid (Factory) with GLM Coding Plan
# =============================================================================

# IMPORTANT: Droid CLI uses browser-based authentication (TUI login)
# NO Factory API Key needed! Just run `droid` once and login via browser.
# The CLI stores auth tokens in ~/.factory/ automatically.

# Droid CLI path (optional, defaults to 'droid' on PATH)
# DROID_BIN=/home/appuser/.local/bin/droid

# Droid Model Configuration
# If using GLM-4.7 via Z.AI, configure in ~/.factory/config.json (see below)
# Available built-in models: sonnet, opus, haiku, droid-core
# Custom models: glm-4.7 (requires config.json setup)
DROID_MODEL=glm-4.7

# Droid Reasoning Effort: off | none | low | medium | high
DROID_REASONING_EFFORT=medium

# Droid Spec Mode: true | false
DROID_USE_SPEC=false

# Droid Autonomy Level: normal | low | medium | high
# WARNING: 'high' allows most tool executions without confirmation
DROID_DEFAULT_AUTO=medium

# Droid Timeout Configuration
# Maximum timeout in milliseconds (default: 900000 = 15 minutes)
# Timeout scales dynamically based on prompt complexity:
#   Base: 5 min + 2 min per 5000 chars, capped at this max
# DROID_MAX_TIMEOUT_MS=900000

# =============================================================================
# GLM-4.7 Configuration (Z.AI Coding Plan)
# =============================================================================
# 
# GLM-4.7 requires a separate config file: ~/.factory/config.json
# 
# 1. Get your Z.AI API Key from: https://z.ai/manage-apikey/apikey-list
# 2. Subscribe to GLM Coding Plan: https://z.ai/subscribe
# 3. Create ~/.factory/config.json with:
#
# {
#   "custom_models": [
#     {
#       "model_display_name": "GLM-4.7 [Z.AI Coding Plan]",
#       "model": "glm-4.7",
#       "base_url": "https://api.z.ai/api/coding/paas/v4",
#       "api_key": "YOUR_ZAI_API_KEY",
#       "provider": "generic-chat-completion-api",
#       "max_tokens": 131072
#     }
#   ]
# }
#
# IMPORTANT: GLM Coding Plan uses: https://api.z.ai/api/coding/paas/v4
# Standard Z.AI plan uses: https://api.z.ai/api/paas/v4
# =============================================================================

# =============================================================================
# GitHub Integration
# =============================================================================

# GitHub Token (for repository cloning and GitHub CLI)
# Generate at: https://github.com/settings/tokens (select 'repo' scope)
# 
# RECOMMENDED: Use fine-grained tokens (github_pat_...) for better security
# Required scopes:
#   - repo (for private repository access)
#   - user:email (for auto-configuring git identity)
#
# Token types:
#   - github_pat_... = Fine-grained (recommended, more secure)
#   - ghp_... = Classic (broader access)
#
GH_TOKEN=ghp_...
GITHUB_TOKEN=ghp_...  # Same as GH_TOKEN, used by adapter

# GitHub Webhooks (optional, for GitHub issue/PR integration)
# Generate a random secret: openssl rand -hex 32
WEBHOOK_SECRET=your_random_secret_string

# =============================================================================
# Git Identity (Optional - can be auto-configured via /github-setup)
# =============================================================================
# If not set, users can configure via:
#   /github-setup <token> - Auto-fetches from GitHub API
#   /git-identity <name> <email> - Manual configuration
#
# GIT_USER_NAME="Your Name"
# GIT_USER_EMAIL="your@email.com"

# =============================================================================
# Telegram Bot
# =============================================================================

# Telegram Bot Token (required)
# Get from @BotFather on Telegram
TELEGRAM_BOT_TOKEN=123456789:ABCdefGHI...

# Telegram Streaming Mode: stream | batch
# stream = real-time message updates (recommended)
# batch = single message after completion
TELEGRAM_STREAMING_MODE=stream

# Telegram Allowlist (optional)
# Comma-separated Telegram user IDs to restrict access
# Get your ID from @userinfobot on Telegram
TELEGRAM_ALLOWLIST=123456789,987654321

# Telegram Startup Retry Configuration
# Number of retry attempts for network errors (ETIMEDOUT, ECONNRESET, etc.)
TELEGRAM_START_RETRIES=5

# Base delay between retries in milliseconds (uses exponential backoff)
# Actual delays: 5s, 10s, 20s, 40s, 80s...
TELEGRAM_RETRY_DELAY_MS=5000

# =============================================================================
# Server Configuration
# =============================================================================

# HTTP Server Port
PORT=3000

# Workspace Path (for Docker volume mount)
WORKSPACE_PATH=./workspace

# Maximum Concurrent Conversations
MAX_CONCURRENT_CONVERSATIONS=10

# =============================================================================
# GitHub Webhooks Streaming Mode (optional)
# =============================================================================
GITHUB_STREAMING_MODE=batch

# =============================================================================
# Preview Deployment - Surge.sh (FREE)
# =============================================================================

# Surge.sh credentials for automated deployments
# Setup steps:
# 1. Install: npm install -g surge
# 2. Login: surge login (interactive, do once locally)
# 3. Get token: surge token
# 4. Add credentials below

# Your Surge.sh email (used during login)
SURGE_LOGIN=your@email.com

# Your Surge.sh token (get via: surge token)
SURGE_TOKEN=your_surge_token_here

# =============================================================================
# Preview Deployment - Other Providers (Optional)
# =============================================================================

# Vercel (FREE tier) - Sophisticated Deployment System
# Get token: https://vercel.com/account/tokens
# 
# Server-level token (used as fallback when users don't set their own)
# VERCEL_TOKEN=your_vercel_token
#
# Telegram Commands:
#   /vercel_setup <token>     - Set user token
#   /vercel [dir]             - Deploy to preview
#   /vercel_prod [dir]        - Deploy to production
#   /vercel_debug on|off      - Toggle verbose output
#   /vercel_env add KEY=value - Add environment variable
#   /vercel_archive on|off    - Enable for large projects (1000+ files)
#   /vercel_regions sfo1,iad1 - Set deployment regions
#
# Features:
#   - Per-user token management
#   - Preview vs Production deployments
#   - Build-time and runtime environment variables
#   - Debug mode with verbose output
#   - Archive mode for large projects
#   - Region selection
#   - Project and team/org configuration

# Netlify (FREE tier)
# Get token: https://app.netlify.com/user/applications#personal-access-tokens
# NETLIFY_AUTH_TOKEN=your_netlify_token

# Cloudflare Pages (FREE)
# Get token: https://dash.cloudflare.com/profile/api-tokens
# CLOUDFLARE_API_TOKEN=your_cloudflare_token

# =============================================================================
# VPS Keep-Alive System
# =============================================================================

# Enable/disable the keep-alive system (default: true)
KEEP_ALIVE_ENABLED=true

# Heartbeat interval in milliseconds (default: 60000 = 1 minute)
KEEP_ALIVE_INTERVAL=60000

# Enable systemd watchdog notifications (default: true)
# Only works when running under systemd with WatchdogSec configured
WATCHDOG_ENABLED=true

# Enable external URL pings for network activity (default: false)
# Useful for VPS providers that detect inactivity by network traffic
KEEP_ALIVE_EXTERNAL_PING=false

# Comma-separated list of external URLs to ping (optional)
# Example: https://api.telegram.org,https://www.google.com
KEEP_ALIVE_EXTERNAL_URLS=

# =============================================================================
# NOVA Async Job Queue System - "Infinite Patience" Architecture
# =============================================================================

# Enable async job routing for complex tasks (default: false)
# When enabled, complex tasks are queued for background processing
# instead of blocking the Telegram response
# 
# RECOMMENDED: Set to true for production to handle complex NOVA missions
NOVA_ASYNC_JOBS_ENABLED=true

# Complexity threshold for async routing: quick | medium | complex (default: complex)
# - quick: Route all tasks to job queue (not recommended)
# - medium: Route medium and complex tasks to job queue
# - complex: Only route complex tasks to job queue (recommended)
NOVA_ASYNC_THRESHOLD=complex

# Job worker poll interval in milliseconds (default: 5000 = 5 seconds)
JOB_POLL_INTERVAL_MS=5000

# Maximum concurrent jobs the worker can process (default: 3)
JOB_MAX_CONCURRENT=3
